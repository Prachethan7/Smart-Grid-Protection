{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "086a5ada",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before balancing:\n",
      "delay\n",
      "normal                    2759425\n",
      "random_replay               39000\n",
      "high_StNum                  39000\n",
      "injection                   39000\n",
      "inverse_replay              26033\n",
      "poisoned_high_rate          18574\n",
      "masquerade_fake_normal      17419\n",
      "masquerade_fake_fault       17287\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_10212\\2071924192.py:41: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: resample(x, replace=False, n_samples=min_class_size, random_state=42))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After balancing:\n",
      "class\n",
      "0    17287\n",
      "1    17287\n",
      "2    17287\n",
      "3    17287\n",
      "4    17287\n",
      "5    17287\n",
      "6    17287\n",
      "7    17287\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final shapes:\n",
      "X: (138296, 56)\n",
      "y: (138296,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.utils import resample\n",
    "import joblib\n",
    "\n",
    "def preprocess_train(train_path, target_column=\"delay\"):\n",
    "    df = pd.read_csv(train_path)\n",
    "\n",
    "    # Drop metadata / non-predictive columns\n",
    "    drop_cols = [\n",
    "        \"id\", \"Time\", \"t\", \"GooseTimestamp\", \"timestampDiff\", \"tDiff\",\n",
    "        \"ethDst\", \"ethSrc\", \"goID\", \"datSet\", \"gocbRef\", \"TPID\", \"ethType\"\n",
    "    ]\n",
    "    df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "\n",
    "    # Separate features and target\n",
    "    y_raw = df[target_column].astype(str)\n",
    "    X = df.drop(columns=[target_column])\n",
    "\n",
    "    # Encode categorical feature columns\n",
    "    encoders = {}\n",
    "    for col in X.select_dtypes(include=[\"object\"]).columns:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "        encoders[col] = le\n",
    "\n",
    "    # Encode target labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y_raw)\n",
    "\n",
    "    print(\"Before balancing:\")\n",
    "    print(pd.Series(y_raw).value_counts())\n",
    "\n",
    "    # Downsample\n",
    "    df_balanced = X.copy()\n",
    "    df_balanced[\"class\"] = y_encoded\n",
    "    min_class_size = df_balanced[\"class\"].value_counts().min()\n",
    "    df_resampled = (\n",
    "        df_balanced.groupby(\"class\", group_keys=False)\n",
    "        .apply(lambda x: resample(x, replace=False, n_samples=min_class_size, random_state=42))\n",
    "    )\n",
    "\n",
    "    print(\"\\nAfter balancing:\")\n",
    "    print(df_resampled[\"class\"].value_counts())\n",
    "\n",
    "    # Separate back into X and y\n",
    "    y_encoded = df_resampled[\"class\"].values\n",
    "    X = df_resampled.drop(columns=[\"class\"])\n",
    "\n",
    "    # Scale numeric features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Save preprocessing objects\n",
    "    joblib.dump(encoders, \"encoders.pkl\")\n",
    "    joblib.dump(label_encoder, \"label_encoder.pkl\")\n",
    "    joblib.dump(scaler, \"scaler.pkl\")\n",
    "    np.save(\"feature_columns.npy\", X.columns)\n",
    "\n",
    "    print(\"\\nFinal shapes:\")\n",
    "    print(\"X:\", X_scaled.shape)\n",
    "    print(\"y:\", y_encoded.shape)\n",
    "\n",
    "    return X_scaled, y_encoded\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_train, y_train = preprocess_train(\"train.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "07a232b6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final shapes:\n",
      "X_test: (2955648, 56)\n",
      "y_test: (2955648,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "def preprocess_test(test_path, target_column=\"delay\"):\n",
    "    df_test = pd.read_csv(test_path)\n",
    "\n",
    "    # Drop metadata / non-predictive columns\n",
    "    drop_cols = [\n",
    "        \"id\", \"Time\", \"t\", \"GooseTimestamp\", \"timestampDiff\", \"tDiff\",\n",
    "        \"ethDst\", \"ethSrc\", \"goID\", \"datSet\", \"gocbRef\", \"TPID\", \"ethType\"\n",
    "    ]\n",
    "    df_test = df_test.drop(columns=[c for c in drop_cols if c in df_test.columns])\n",
    "\n",
    "    # Separate features and target (if available)\n",
    "    if target_column in df_test.columns:\n",
    "        y_test_raw = df_test[target_column].astype(str)\n",
    "        X_test = df_test.drop(columns=[target_column])\n",
    "    else:\n",
    "        y_test_raw = None\n",
    "        X_test = df_test.copy()\n",
    "\n",
    "    # Load saved encoders & scaler\n",
    "    encoders = joblib.load(\"encoders.pkl\")\n",
    "    label_encoder = joblib.load(\"label_encoder.pkl\")\n",
    "    scaler = joblib.load(\"scaler.pkl\")\n",
    "    feature_columns = np.load(\"feature_columns.npy\", allow_pickle=True)\n",
    "\n",
    "    # Encode categorical columns using TRAIN encoders\n",
    "    for col, le in encoders.items():\n",
    "        if col in X_test.columns:\n",
    "            unseen = set(X_test[col].astype(str)) - set(le.classes_)\n",
    "            if unseen:\n",
    "                le.classes_ = np.append(le.classes_, list(unseen))\n",
    "            X_test[col] = le.transform(X_test[col].astype(str))\n",
    "\n",
    "    # Ensure same feature alignment\n",
    "    X_test = X_test.reindex(columns=feature_columns, fill_value=0)\n",
    "\n",
    "    # Scale numeric features\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Encode target (if available)\n",
    "    if y_test_raw is not None:\n",
    "        try:\n",
    "            y_test_encoded = label_encoder.transform(y_test_raw)\n",
    "        except ValueError:\n",
    "            print(\"⚠️ Warning: Test set contains unseen labels!\")\n",
    "            y_test_encoded = None\n",
    "        y_test_labels = label_encoder.classes_\n",
    "    else:\n",
    "        y_test_encoded, y_test_labels = None, None\n",
    "\n",
    "    print(\"\\nFinal shapes:\")\n",
    "    print(\"X_test:\", X_test_scaled.shape)\n",
    "    if y_test_encoded is not None:\n",
    "        print(\"y_test:\", y_test_encoded.shape)\n",
    "    else:\n",
    "        print(\"No target column or unseen target labels in test dataset\")\n",
    "\n",
    "    return X_test_scaled, y_test_encoded\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_test, y_test = preprocess_test(\"test.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "c5d77216",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training Logistic Regression...\n",
      "Training Random Forest...\n",
      "Training SVM...\n",
      "Training KNN...\n",
      "\n",
      "Benchmarking Results (Multi-class Models):\n",
      "                     Accuracy  Precision    Recall  F1-score       AUC\n",
      "Logistic Regression  0.850058   0.972749  0.850058  0.899508  0.984347\n",
      "Random Forest        0.997731   0.997856  0.997731  0.997759  0.999854\n",
      "SVM                  0.926962   0.979197  0.926962  0.948145  0.992178\n",
      "KNN                  0.922804   0.970008  0.922804  0.941667  0.966935\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score, roc_auc_score\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.ensemble import RandomForestClassifier\n",
    "from sklearn.svm import SVC\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import label_binarize\n",
    "import numpy as np\n",
    "\n",
    "# =====================================================\n",
    "# Function for Multi-class Benchmarking\n",
    "# =====================================================\n",
    "def benchmark_models_multiclass(X_train, y_train, X_test, y_test):\n",
    "    models = {\n",
    "        \"Logistic Regression\": LogisticRegression(max_iter=1000),\n",
    "        \"Random Forest\": RandomForestClassifier(n_estimators=100, random_state=42),\n",
    "        \"SVM\": SVC(probability=True, random_state=42),\n",
    "        \"KNN\": KNeighborsClassifier(),\n",
    "    }\n",
    "    \n",
    "    results = {}\n",
    "    classes = np.unique(y_train)\n",
    "    \n",
    "    for name, model in models.items():\n",
    "        print(f\"Training {name}...\")\n",
    "        model.fit(X_train, y_train)\n",
    "        \n",
    "        y_pred = model.predict(X_test)\n",
    "        \n",
    "        # Handle metrics for multiclass\n",
    "        results[name] = {\n",
    "            \"Accuracy\": accuracy_score(y_test, y_pred),\n",
    "            \"Precision\": precision_score(y_test, y_pred, average=\"weighted\"),\n",
    "            \"Recall\": recall_score(y_test, y_pred, average=\"weighted\"),\n",
    "            \"F1-score\": f1_score(y_test, y_pred, average=\"weighted\")\n",
    "        }\n",
    "        \n",
    "        # AUC (one-vs-rest) if model supports probabilities\n",
    "        if hasattr(model, \"predict_proba\"):\n",
    "            y_prob = model.predict_proba(X_test)\n",
    "            y_test_bin = label_binarize(y_test, classes=classes)\n",
    "            try:\n",
    "                auc = roc_auc_score(y_test_bin, y_prob, average=\"weighted\", multi_class=\"ovr\")\n",
    "                results[name][\"AUC\"] = auc\n",
    "            except:\n",
    "                results[name][\"AUC\"] = None\n",
    "        else:\n",
    "            results[name][\"AUC\"] = None\n",
    "    \n",
    "    return pd.DataFrame(results).T\n",
    "\n",
    "# =====================================================\n",
    "# Example usage\n",
    "# =====================================================\n",
    "# Assuming you already have: X_train, y_train, X_test, y_test\n",
    "results_df = benchmark_models_multiclass(X_train, y_train, X_test, y_test)\n",
    "\n",
    "print(\"\\nBenchmarking Results (Multi-class Models):\")\n",
    "print(results_df)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
