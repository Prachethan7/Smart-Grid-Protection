{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "7844aa06",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Before balancing:\n",
      "delay\n",
      "normal                    2759425\n",
      "random_replay               39000\n",
      "high_StNum                  39000\n",
      "injection                   39000\n",
      "inverse_replay              26033\n",
      "poisoned_high_rate          18574\n",
      "masquerade_fake_normal      17419\n",
      "masquerade_fake_fault       17287\n",
      "Name: count, dtype: int64\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Users\\Asus\\AppData\\Local\\Temp\\ipykernel_20092\\179856920.py:41: FutureWarning: DataFrameGroupBy.apply operated on the grouping columns. This behavior is deprecated, and in a future version of pandas the grouping columns will be excluded from the operation. Either pass `include_groups=False` to exclude the groupings or explicitly select the grouping columns after groupby to silence this warning.\n",
      "  .apply(lambda x: resample(x, replace=False, n_samples=min_class_size, random_state=42))\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "After balancing:\n",
      "class\n",
      "0    17287\n",
      "1    17287\n",
      "2    17287\n",
      "3    17287\n",
      "4    17287\n",
      "5    17287\n",
      "6    17287\n",
      "7    17287\n",
      "Name: count, dtype: int64\n",
      "\n",
      "Final shapes:\n",
      "X: (138296, 56)\n",
      "y: (138296,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.preprocessing import LabelEncoder, StandardScaler\n",
    "from sklearn.utils import resample\n",
    "import joblib\n",
    "\n",
    "def preprocess_train(train_path, target_column=\"delay\"):\n",
    "    df = pd.read_csv(train_path)\n",
    "\n",
    "    # Drop metadata / non-predictive columns\n",
    "    drop_cols = [\n",
    "        \"id\", \"Time\", \"t\", \"GooseTimestamp\", \"timestampDiff\", \"tDiff\",\n",
    "        \"ethDst\", \"ethSrc\", \"goID\", \"datSet\", \"gocbRef\", \"TPID\", \"ethType\"\n",
    "    ]\n",
    "    df = df.drop(columns=[c for c in drop_cols if c in df.columns])\n",
    "\n",
    "    # Separate features and target\n",
    "    y_raw = df[target_column].astype(str)\n",
    "    X = df.drop(columns=[target_column])\n",
    "\n",
    "    # Encode categorical feature columns\n",
    "    encoders = {}\n",
    "    for col in X.select_dtypes(include=[\"object\"]).columns:\n",
    "        le = LabelEncoder()\n",
    "        X[col] = le.fit_transform(X[col].astype(str))\n",
    "        encoders[col] = le\n",
    "\n",
    "    # Encode target labels\n",
    "    label_encoder = LabelEncoder()\n",
    "    y_encoded = label_encoder.fit_transform(y_raw)\n",
    "\n",
    "    print(\"Before balancing:\")\n",
    "    print(pd.Series(y_raw).value_counts())\n",
    "\n",
    "    # Downsample\n",
    "    df_balanced = X.copy()\n",
    "    df_balanced[\"class\"] = y_encoded\n",
    "    min_class_size = df_balanced[\"class\"].value_counts().min()\n",
    "    df_resampled = (\n",
    "        df_balanced.groupby(\"class\", group_keys=False)\n",
    "        .apply(lambda x: resample(x, replace=False, n_samples=min_class_size, random_state=42))\n",
    "    )\n",
    "\n",
    "    print(\"\\nAfter balancing:\")\n",
    "    print(df_resampled[\"class\"].value_counts())\n",
    "\n",
    "    # Separate back into X and y\n",
    "    y_encoded = df_resampled[\"class\"].values\n",
    "    X = df_resampled.drop(columns=[\"class\"])\n",
    "\n",
    "    # Scale numeric features\n",
    "    scaler = StandardScaler()\n",
    "    X_scaled = scaler.fit_transform(X)\n",
    "\n",
    "    # Save preprocessing objects\n",
    "    joblib.dump(encoders, \"encoders.pkl\")\n",
    "    joblib.dump(label_encoder, \"label_encoder.pkl\")\n",
    "    joblib.dump(scaler, \"scaler.pkl\")\n",
    "    np.save(\"feature_columns.npy\", X.columns)\n",
    "\n",
    "    print(\"\\nFinal shapes:\")\n",
    "    print(\"X:\", X_scaled.shape)\n",
    "    print(\"y:\", y_encoded.shape)\n",
    "\n",
    "    return X_scaled, y_encoded\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_train, y_train = preprocess_train(\"train.csv\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ece409dd",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final shapes:\n",
      "X_test: (2955648, 56)\n",
      "y_test: (2955648,)\n"
     ]
    }
   ],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import joblib\n",
    "\n",
    "def preprocess_test(test_path, target_column=\"delay\"):\n",
    "    df_test = pd.read_csv(test_path)\n",
    "\n",
    "    # Drop metadata / non-predictive columns\n",
    "    drop_cols = [\n",
    "        \"id\", \"Time\", \"t\", \"GooseTimestamp\", \"timestampDiff\", \"tDiff\",\n",
    "        \"ethDst\", \"ethSrc\", \"goID\", \"datSet\", \"gocbRef\", \"TPID\", \"ethType\"\n",
    "    ]\n",
    "    df_test = df_test.drop(columns=[c for c in drop_cols if c in df_test.columns])\n",
    "\n",
    "    # Separate features and target (if available)\n",
    "    if target_column in df_test.columns:\n",
    "        y_test_raw = df_test[target_column].astype(str)\n",
    "        X_test = df_test.drop(columns=[target_column])\n",
    "    else:\n",
    "        y_test_raw = None\n",
    "        X_test = df_test.copy()\n",
    "\n",
    "    # Load saved encoders & scaler\n",
    "    encoders = joblib.load(\"encoders.pkl\")\n",
    "    label_encoder = joblib.load(\"label_encoder.pkl\")\n",
    "    scaler = joblib.load(\"scaler.pkl\")\n",
    "    feature_columns = np.load(\"feature_columns.npy\", allow_pickle=True)\n",
    "\n",
    "    # Encode categorical columns using TRAIN encoders\n",
    "    for col, le in encoders.items():\n",
    "        if col in X_test.columns:\n",
    "            unseen = set(X_test[col].astype(str)) - set(le.classes_)\n",
    "            if unseen:\n",
    "                le.classes_ = np.append(le.classes_, list(unseen))\n",
    "            X_test[col] = le.transform(X_test[col].astype(str))\n",
    "\n",
    "    # Ensure same feature alignment\n",
    "    X_test = X_test.reindex(columns=feature_columns, fill_value=0)\n",
    "\n",
    "    # Scale numeric features\n",
    "    X_test_scaled = scaler.transform(X_test)\n",
    "\n",
    "    # Encode target (if available)\n",
    "    if y_test_raw is not None:\n",
    "        try:\n",
    "            y_test_encoded = label_encoder.transform(y_test_raw)\n",
    "        except ValueError:\n",
    "            print(\"⚠️ Warning: Test set contains unseen labels!\")\n",
    "            y_test_encoded = None\n",
    "        y_test_labels = label_encoder.classes_\n",
    "    else:\n",
    "        y_test_encoded, y_test_labels = None, None\n",
    "\n",
    "    print(\"\\nFinal shapes:\")\n",
    "    print(\"X_test:\", X_test_scaled.shape)\n",
    "    if y_test_encoded is not None:\n",
    "        print(\"y_test:\", y_test_encoded.shape)\n",
    "    else:\n",
    "        print(\"No target column or unseen target labels in test dataset\")\n",
    "\n",
    "    return X_test_scaled, y_test_encoded\n",
    "\n",
    "if __name__ == \"__main__\":\n",
    "    X_test, y_test = preprocess_test(\"test.csv\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7c886634",
   "metadata": {},
   "source": [
    "Model Design"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "5eee5f65",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 1/25\n",
      "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m8s\u001b[0m 12ms/step - accuracy: 0.8304 - loss: 0.4811 - val_accuracy: 0.9334 - val_loss: 0.2005\n",
      "Epoch 2/25\n",
      "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.9153 - loss: 0.2392 - val_accuracy: 0.9523 - val_loss: 0.1372\n",
      "Epoch 3/25\n",
      "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9342 - loss: 0.1887 - val_accuracy: 0.9681 - val_loss: 0.1063\n",
      "Epoch 4/25\n",
      "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9462 - loss: 0.1561 - val_accuracy: 0.9671 - val_loss: 0.1011\n",
      "Epoch 5/25\n",
      "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9539 - loss: 0.1361 - val_accuracy: 0.9774 - val_loss: 0.0728\n",
      "Epoch 6/25\n",
      "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.9596 - loss: 0.1189 - val_accuracy: 0.9812 - val_loss: 0.0655\n",
      "Epoch 7/25\n",
      "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.9635 - loss: 0.1086 - val_accuracy: 0.9827 - val_loss: 0.0565\n",
      "Epoch 8/25\n",
      "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.9666 - loss: 0.0993 - val_accuracy: 0.9872 - val_loss: 0.0478\n",
      "Epoch 9/25\n",
      "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.9692 - loss: 0.0924 - val_accuracy: 0.9827 - val_loss: 0.0511\n",
      "Epoch 10/25\n",
      "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9725 - loss: 0.0858 - val_accuracy: 0.9875 - val_loss: 0.0399\n",
      "Epoch 11/25\n",
      "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9738 - loss: 0.0798 - val_accuracy: 0.9893 - val_loss: 0.0366\n",
      "Epoch 12/25\n",
      "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.9758 - loss: 0.0766 - val_accuracy: 0.9895 - val_loss: 0.0353\n",
      "Epoch 13/25\n",
      "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9766 - loss: 0.0726 - val_accuracy: 0.9894 - val_loss: 0.0348\n",
      "Epoch 14/25\n",
      "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.9781 - loss: 0.0698 - val_accuracy: 0.9900 - val_loss: 0.0320\n",
      "Epoch 15/25\n",
      "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m10s\u001b[0m 12ms/step - accuracy: 0.9793 - loss: 0.0660 - val_accuracy: 0.9897 - val_loss: 0.0330\n",
      "Epoch 16/25\n",
      "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.9800 - loss: 0.0639 - val_accuracy: 0.9884 - val_loss: 0.0356\n",
      "Epoch 17/25\n",
      "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.9794 - loss: 0.0646 - val_accuracy: 0.9919 - val_loss: 0.0278\n",
      "Epoch 18/25\n",
      "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9808 - loss: 0.0596 - val_accuracy: 0.9914 - val_loss: 0.0298\n",
      "Epoch 19/25\n",
      "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 11ms/step - accuracy: 0.9816 - loss: 0.0579 - val_accuracy: 0.9920 - val_loss: 0.0269\n",
      "Epoch 20/25\n",
      "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.9823 - loss: 0.0562 - val_accuracy: 0.9925 - val_loss: 0.0271\n",
      "Epoch 21/25\n",
      "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.9822 - loss: 0.0554 - val_accuracy: 0.9908 - val_loss: 0.0281\n",
      "Epoch 22/25\n",
      "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.9831 - loss: 0.0534 - val_accuracy: 0.9917 - val_loss: 0.0272\n",
      "Epoch 23/25\n",
      "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.9828 - loss: 0.0539 - val_accuracy: 0.9919 - val_loss: 0.0265\n",
      "Epoch 24/25\n",
      "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.9833 - loss: 0.0530 - val_accuracy: 0.9923 - val_loss: 0.0263\n",
      "Epoch 25/25\n",
      "\u001b[1m433/433\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m5s\u001b[0m 12ms/step - accuracy: 0.9841 - loss: 0.0510 - val_accuracy: 0.9919 - val_loss: 0.0256\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:You are saving your model as an HDF5 file via `model.save()` or `keras.saving.save_model(model)`. This file format is considered legacy. We recommend using instead the native Keras format, e.g. `model.save('my_model.keras')` or `keras.saving.save_model(model, 'my_model.keras')`. \n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import Dense, Dropout, BatchNormalization\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.utils.class_weight import compute_class_weight\n",
    "\n",
    "# Split data (use y_encoded)\n",
    "X_train, X_test, y_train, y_test = train_test_split(\n",
    "    X_train, y_train, test_size=0.2, random_state=42, stratify=y_train\n",
    ")\n",
    "\n",
    "n_features = X_train.shape[1]              # number of features\n",
    "n_classes = len(np.unique(y_train))  # number of unique attack/normal classes\n",
    "\n",
    "# Compute class weights (handle imbalance)\n",
    "class_weights = compute_class_weight(\n",
    "    class_weight=\"balanced\", \n",
    "    classes=np.unique(y_train), \n",
    "    y=y_train\n",
    ")\n",
    "class_weights = dict(zip(np.unique(y_train), class_weights))\n",
    "\n",
    "# Model\n",
    "model = Sequential([\n",
    "    tf.keras.Input(shape=(n_features,)),   # Input layer\n",
    "    Dense(256, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.4),\n",
    "\n",
    "    Dense(128, activation='relu'),\n",
    "    BatchNormalization(),\n",
    "    Dropout(0.3),\n",
    "\n",
    "    Dense(64, activation='relu'),\n",
    "    Dropout(0.2),\n",
    "\n",
    "    Dense(n_classes, activation='softmax')  # final layer matches class count\n",
    "])\n",
    "\n",
    "# Compile\n",
    "model.compile(\n",
    "    optimizer=tf.keras.optimizers.Adam(learning_rate=0.001),\n",
    "    loss='sparse_categorical_crossentropy',   # since y_train are integers\n",
    "    metrics=['accuracy']\n",
    ")\n",
    "\n",
    "# Train\n",
    "history = model.fit(\n",
    "    X_train, y_train,\n",
    "    validation_data=(X_test, y_test),\n",
    "    epochs=25,\n",
    "    batch_size=256,\n",
    "    class_weight=class_weights,\n",
    "    verbose=1\n",
    ")\n",
    "\n",
    "model.save(\"smart_grid_model.h5\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87c75fb6",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Final shapes:\n",
      "X_test: (2955648, 56)\n",
      "y_test: (2955648,)\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "WARNING:absl:Compiled the loaded model, but the compiled metrics have yet to be built. `model.compile_metrics` will be empty until you train or evaluate the model.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1m92364/92364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m265s\u001b[0m 3ms/step - accuracy: 0.9695 - loss: 0.0946\n",
      "\n",
      "Test Loss: 0.0946\n",
      "Test Accuracy: 0.9695\n",
      "\u001b[1m92364/92364\u001b[0m \u001b[32m━━━━━━━━━━━━━━━━━━━━\u001b[0m\u001b[37m\u001b[0m \u001b[1m165s\u001b[0m 2ms/step\n",
      "\n",
      "Classification Report:\n",
      "                        precision    recall  f1-score   support\n",
      "\n",
      "            high_StNum       0.99      1.00      1.00     39000\n",
      "             injection       0.99      0.97      0.98     39000\n",
      "        inverse_replay       0.75      0.76      0.75     30319\n",
      " masquerade_fake_fault       0.19      0.85      0.31     17200\n",
      "masquerade_fake_normal       0.93      1.00      0.97     17420\n",
      "                normal       1.00      0.97      0.98   2755139\n",
      "    poisoned_high_rate       0.93      0.99      0.96     18570\n",
      "         random_replay       0.76      0.99      0.86     39000\n",
      "\n",
      "              accuracy                           0.97   2955648\n",
      "             macro avg       0.82      0.94      0.85   2955648\n",
      "          weighted avg       0.99      0.97      0.98   2955648\n",
      "\n",
      "\n",
      "Confusion Matrix:\n",
      "[[  38964      33       0       0       0       0       0       3]\n",
      " [    221   37917       9     237       1     601       0      14]\n",
      " [      0      72   22962    3179       0    2047     147    1912]\n",
      " [      0       0       7   14631       0    1294    1243      25]\n",
      " [      0       0       0       0   17419       1       0       0]\n",
      " [      0     113    7429   59908    1210 2676408       0   10071]\n",
      " [      0       0     115     131       0       0   18324       0]\n",
      " [      0      17     131      59       1      31       1   38760]]\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "from sklearn.metrics import classification_report, confusion_matrix\n",
    "from tensorflow.keras.models import load_model\n",
    "from tensorflow.keras.utils import to_categorical\n",
    "import joblib\n",
    "\n",
    "\n",
    "# Your existing preprocess_test function\n",
    "X_test, y_test = preprocess_test(\"test.csv\")  # runs preprocessing once\n",
    "\n",
    "# Load trained model\n",
    "model = load_model(\"smart_grid_model .h5\")\n",
    "\n",
    "# Prepare one-hot labels if needed\n",
    "n_classes = len(np.unique(y_test))\n",
    "y_test_onehot = to_categorical(y_test, num_classes=n_classes) if y_test is not None else None\n",
    "\n",
    "# Evaluate model\n",
    "loss, acc = model.evaluate(X_test, y_test, verbose=1)\n",
    "print(f\"\\nTest Loss: {loss:.4f}\")\n",
    "print(f\"Test Accuracy: {acc:.4f}\")\n",
    "\n",
    "# Predictions\n",
    "y_pred_probs = model.predict(X_test)\n",
    "y_pred = np.argmax(y_pred_probs, axis=1)\n",
    "\n",
    "# Load label classes for printing\n",
    "label_encoder = joblib.load(\"label_encoder.pkl\")\n",
    "y_labels = label_encoder.classes_\n",
    "\n",
    "# Classification report & confusion matrix\n",
    "print(\"\\nClassification Report:\")\n",
    "print(classification_report(y_test, y_pred, target_names=y_labels))\n",
    "\n",
    "print(\"\\nConfusion Matrix:\")\n",
    "print(confusion_matrix(y_test, y_pred))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bd38ef77",
   "metadata": {},
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "pers",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
